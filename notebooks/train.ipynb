{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ishanawal/miniforge3/envs/myenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: True\n",
      "TF: False\n",
      "Nepali lines: 317120\n",
      "English lines: 317120\n",
      "Sample item in pairs: ('‡§ï‡§æ‡§∞‡•ç‡§Ø‡§∂‡§æ‡§≤‡§æ ‡§™‡•ç‡§≤‡§æ‡§®‡§ø‡§ô ‡§ò‡§æ‡§Æ ‡§∞ ‡§π‡§æ‡§µ‡§æ‡§ï‡•ã ‡§¶‡§ø‡§∂‡§æ‡§Æ‡§æ ‡§Ü‡§ß‡§æ‡§∞‡§ø‡§§ ‡§•‡§ø‡§Ø‡•ã, ‡§∏‡§Æ‡•ç‡§™‡•Ç‡§∞‡•ç‡§£ ‡§®‡§ø‡§∞‡•ç‡§Æ‡§æ‡§§‡§æ‡§≤‡•á ‡§∏‡•å‡§∞‡•ç‡§Ø ‡§ä‡§∞‡•ç‡§ú‡§æ‡§ï‡•ã ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ó‡§∞‡•ç‡§® ‡§∏‡§ï‡•ç‡§•‡•á‡•§M&Z Furniture ‡§≤‡•á ‡§§‡§æ‡§™‡§ï‡•ç‡§∞‡§Æ ‡§∞ ‡§∏‡§æ‡§™‡•á‡§ï‡•ç‡§∑‡§ø‡§ï ‡§Ü‡§∞‡•ç‡§¶‡•ç‡§∞‡§§‡§æ ‡§ï‡§æ‡§Ø‡§Æ ‡§∞‡§æ‡§ñ‡•ç‡§® ‡§≠‡•Ç‡§Æ‡§ø‡§ó‡§§ ‡§™‡§æ‡§®‡•Ä‡§ï‡•ã ‡§™‡§∞‡§ø‡§∏‡§Ç‡§ö‡§∞‡§£ ‡§Ö‡§™‡§®‡§æ‡§Ø‡•ã, ‡§∞ ‡§ß‡•Å‡§≤‡•ã ‡§∏‡§ô‡•ç‡§ï‡§≤‡§® ‡§™‡•ç‡§∞‡§£‡§æ‡§≤‡•Ä‡§π‡§∞‡•Ç‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§ß‡•Å‡§≤‡•ã-‡§∞‡§π‡§ø‡§§ ‡§µ‡§æ‡§§‡§æ‡§µ‡§∞‡§£ ‡§∞‡§æ‡§ñ‡•ç‡§Ø‡•ã ‡§ú‡§∏‡§≤‡•á ‡§´‡§ø‡§≤‡•ç‡§ü‡§∞‡§π‡§∞‡•Ç ‡§Æ‡§æ‡§∞‡•ç‡§´‡§§ ‡§π‡§æ‡§µ‡§æ‡§≤‡§æ‡§à ‡§®‡§ø‡§∞‡§®‡•ç‡§§‡§∞ ‡§¨‡§æ‡§π‡§ø‡§∞ ‡§ß‡§ï‡•á‡§≤‡•ç‡§õ ‡§∞ ‡§∏‡§´‡§æ ‡§π‡§æ‡§µ‡§æ‡§Æ‡§æ ‡§™‡•Å‡§®: ‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó ‡§ó‡§∞‡•ç‡§¶‡§õ‡•§', 'Climate-Controlled & Dust-Free Environment Workshop planing was based on sunshine and wind direction, the whole manufacturer could make full use of solar energy. M&Z Furniture adopted underground water circulation to keep temperature and relative humidity, and kept dust-free environment by dust collection systems that constantly push air out through filters and recycle in clean air.')\n",
      "Length of train_data: 253696\n",
      "‚ùå Skipping invalid pair at index 0: PTC ‡§π‡•Ä‡§ü‡§ø‡§Ç‡§ó ‡§ï‡§æ‡§∞‡•ç‡§Ø, ‡§ï‡§Æ ‡§§‡§æ‡§™‡§ï‡•ç‡§∞‡§Æ‡§Æ‡§æ, PTC ‡§á‡§≤‡•á‡§ï‡•ç‡§ü‡•ç‡§∞‡§ø‡§ï ‡§π‡•Ä‡§ü‡§∞ ‡§π‡•Ä‡§ü‡§ø‡§Ç‡§ó, ‡§ö‡§ø‡§∏‡•ã ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞‡§Æ‡§æ ‡§â‡§§‡•ç‡§™‡§æ‡§¶‡§®‡§π‡§∞‡•Ç ‡§™‡§®‡§ø ‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó ‡§ó‡§∞‡•ç‡§® ‡§∏‡§ï‡•ç‡§∑‡§Æ ‡§π‡•Å‡§®‡•á‡§õ ‡§≠‡§®‡•á‡§∞ ‡§∏‡•Å‡§®‡§ø‡§∂‡•ç‡§ö‡§ø‡§§ ‡§ó‡§∞‡•ç‡§®‡•§\n",
      "‚ùå Skipping invalid pair at index 1: PTC heating function, at low temperature, PTC electric heater heating, to ensure that products in the cold area can also be able to use.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "\n",
    "from datasets import DatasetDict, Dataset\n",
    "import random\n",
    "from transformers import is_torch_available, is_tf_available\n",
    "print(\"Torch:\", is_torch_available())\n",
    "print(\"TF:\", is_tf_available())\n",
    "\n",
    "\n",
    "# Dataset FilePaths\n",
    "src_file = \"../dataset/MultiHPLT.en-ne.ne\"\n",
    "tgt_file = \"../dataset/MultiHPLT.en-ne.en\"\n",
    "\n",
    "# Load the full dataset\n",
    "with open(src_file, encoding=\"utf-8\") as f:\n",
    "    nepali = [line.strip() for line in f]\n",
    "\n",
    "with open(tgt_file, encoding=\"utf-8\") as f:\n",
    "    english = [line.strip() for line in f]\n",
    "\n",
    "print(\"Nepali lines:\", len(nepali))\n",
    "print(\"English lines:\", len(english))\n",
    "\n",
    "# Pair and shuffle\n",
    "min_len = min(len(nepali), len(english))\n",
    "data = list(zip(nepali[:min_len], english[:min_len]))\n",
    "random.seed(42)\n",
    "random.shuffle(data)\n",
    "\n",
    "\n",
    "# Split into 80% train, 15% val and 5% test\n",
    "total = len(data)\n",
    "train_data = data[:int(0.8 * total)]\n",
    "print(f\"Sample item in pairs: {train_data[0]}\")\n",
    "print(f\"Length of train_data: {len(train_data)}\")\n",
    "\n",
    "val_data = data[int(0.8 * total):int(0.95 * total)]\n",
    "test_data = data[int(0.95 * total)]\n",
    "\n",
    "\n",
    "# Convertting to Hugging Face datasets\n",
    "def to_dataset(pairs):\n",
    "    cleaned = []\n",
    "    for i, pair in enumerate(pairs):\n",
    "        if isinstance(pair, tuple) and len(pair) == 2:\n",
    "            cleaned.append({\"ne\": pair[0], \"en\": pair[1]})\n",
    "        else:\n",
    "            print(f\"‚ùå Skipping invalid pair at index {i}: {pair}\")\n",
    "    return Dataset.from_dict({\"translation\": cleaned})\n",
    "\n",
    "\n",
    "raw_datasets  = DatasetDict({\n",
    "    \"train\": to_dataset(train_data),\n",
    "    \"val\": to_dataset(val_data),\n",
    "    \"test\": to_dataset(test_data)\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:   0%|          | 0/253696 [00:00<?, ? examples/s]/Users/ishanawal/miniforge3/envs/myenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:4016: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 253696/253696 [00:47<00:00, 5358.39 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47568/47568 [00:08<00:00, 5310.56 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Preparing model\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model_name = \"t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Preprocessing\n",
    "def preprocess(example):\n",
    "    inputs = [\"translate Nepali to English: \" + ex[\"ne\"] for ex in example[\"translation\"]]\n",
    "    targets = [ex[\"en\"] for ex in example[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, padding=\"max_length\", truncation = True)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=128, padding=\"max_length\", truncation = True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize\n",
    "tokenized_datasets = raw_datasets.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ishanawal/miniforge3/envs/myenv/lib/python3.11/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "  0%|          | 0/158560 [02:47<?, ?it/s]\n",
      "  0%|          | 8/158560 [00:45<252:43:44,  5.74s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m      5\u001b[39m training_args = Seq2SeqTrainingArguments(\n\u001b[32m      6\u001b[39m     output_dir=\u001b[33m\"\u001b[39m\u001b[33m./t5-multihplt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m     evaluation_strategy=\u001b[33m\"\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m     use_cpu=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     17\u001b[39m )\n\u001b[32m     19\u001b[39m trainer = Seq2SeqTrainer(\n\u001b[32m     20\u001b[39m     model=model,\n\u001b[32m     21\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m     data_collator=DataCollatorForSeq2Seq(tokenizer, model),\n\u001b[32m     26\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myenv/lib/python3.11/site-packages/transformers/trainer.py:1932\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   1930\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   1931\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1932\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1933\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1934\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1935\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1936\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1937\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myenv/lib/python3.11/site-packages/transformers/trainer.py:2268\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2265\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_step_begin(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m   2267\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.accumulate(model):\n\u001b[32m-> \u001b[39m\u001b[32m2268\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2270\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2271\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2272\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2273\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2274\u001b[39m ):\n\u001b[32m   2275\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2276\u001b[39m     tr_loss += tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myenv/lib/python3.11/site-packages/transformers/trainer.py:3324\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   3322\u001b[39m         scaled_loss.backward()\n\u001b[32m   3323\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3324\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3326\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach() / \u001b[38;5;28mself\u001b[39m.args.gradient_accumulation_steps\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myenv/lib/python3.11/site-packages/accelerate/accelerator.py:2286\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2284\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2285\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2286\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myenv/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    573\u001b[39m         Tensor.backward,\n\u001b[32m    574\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         inputs=inputs,\n\u001b[32m    580\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myenv/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myenv/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "\n",
    "\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./t5-multihplt\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=5,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"val\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, model),\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
