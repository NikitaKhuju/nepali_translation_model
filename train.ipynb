{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ishanawal/miniforge3/envs/myenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': '\"कुनै पनि अन्य सरकारी एजेन्सीले यो जानकारी प्रयोग गर्न सक्दैन, केन्द्रीय सरकार अन्तर्गतका कसैले कुनै पनि हालतमा यो जानकारी पाउँदैनन् र राज्यका अधिकारीहरूमा पनि स्वास्थ्य अधिकारीहरूले मात्र यसलाई प्रयोग गर्न सक्दछन्,\" उनले भने।', 'target': '\"No other government agency can use this information, no one in the commonwealth government at all, and in state authorities, only the health officer can use it.'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "\n",
    "dataset = load_dataset(\"parquet\", data_files={\n",
    "    \"train\": \"data/iamTangsang_dataset/train-00000-of-00001.parquet\",\n",
    "    \"validation\": \"data/iamTangsang_dataset/validation-00000-of-00001.parquet\", \n",
    "    \"test\": \"data/iamTangsang_dataset/test-00000-of-00001.parquet\"\n",
    "    })\n",
    "\n",
    "dataset[\"train\"] = dataset[\"train\"].select(range(10000))       \n",
    "dataset[\"validation\"] = dataset[\"validation\"].select(range(2000))\n",
    "dataset[\"test\"] = dataset[\"test\"].select(range(1000)) \n",
    "\n",
    "dataset = dataset.filter(lambda x: x[\"source\"].strip() != \"\" and x[\"target\"].strip() != \"\")\n",
    "\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
    "\n",
    "tokenizer = MT5Tokenizer.from_pretrained(\"google/mt5-small\", legacy=False)\n",
    "\n",
    "def preprocess(examples):\n",
    "    inputs = [\"translate Nepali to English: \" + ex for ex in examples[\"source\"]]\n",
    "    targets = examples[\"target\"]\n",
    "\n",
    "    model_inputs = tokenizer(inputs, max_length=300, truncation=True, padding=\"longest\")\n",
    "\n",
    "    labels = tokenizer(targets, max_length=300, truncation=True, padding=\"longest\")\n",
    "\n",
    "    model_inputs[\"labels\"] = [\n",
    "        [(token if token != tokenizer.pad_token_id else -100) for token in label]\n",
    "        for label in labels[\"input_ids\"]\n",
    "    ]\n",
    "    return model_inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab size: 250100\n",
      "Max length in batch of 1000: 272\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizer vocab size:\", tokenizer.vocab_size)\n",
    "\n",
    "\n",
    "lengths = [len(tokenizer(\"translate Nepali to English: \" + ex)[\"input_ids\"])\n",
    "           for ex in dataset[\"train\"][\"source\"][:1000]]\n",
    "\n",
    "print(\"Max length in batch of 1000:\", max(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['source', 'target', 'input_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess, batched=True, num_proc=4)\n",
    "tokenized_dataset.set_format(type=\"torch\")\n",
    "print(tokenized_dataset[\"train\"][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total invalid tokens in validation set: 0\n",
      "\n",
      "Total invalid tokens in train set: 0\n",
      "\n",
      "Total invalid tokens in test set: 0\n"
     ]
    }
   ],
   "source": [
    "def find_invalid_tokens(dataset_split, name=\"\"):\n",
    "    invalid_count = 0\n",
    "    for i, ex in enumerate(dataset_split):\n",
    "        for token in ex[\"labels\"]:\n",
    "            if token != -100 and token >= tokenizer.vocab_size:\n",
    "                print(f\"❌ Invalid token ID at index {i} in {name} set: {token}\")\n",
    "                print(\"Target text:\", dataset[\"validation\"][i][\"target\"])\n",
    "                invalid_count += 1\n",
    "    print(f\"\\nTotal invalid tokens in {name} set:\", invalid_count)\n",
    "\n",
    "find_invalid_tokens(tokenized_dataset[\"validation\"], name=\"validation\")\n",
    "find_invalid_tokens(tokenized_dataset[\"train\"], name=\"train\")\n",
    "find_invalid_tokens(tokenized_dataset[\"test\"], name=\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./mt5-npi-en\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    generation_max_length=300,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False, \n",
    "    use_cpu=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2250 [00:00<?, ?it/s]/Users/ishanawal/miniforge3/envs/myenv/lib/python3.11/site-packages/transformers/data/data_collator.py:656: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:281.)\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n",
      " 33%|███▎      | 750/2250 [15:22<26:15,  1.05s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.2881, 'grad_norm': 6.854383945465088, 'learning_rate': 0.00013333333333333334, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 33%|███▎      | 750/2250 [15:37<26:15,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.4174652099609375, 'eval_runtime': 15.5652, 'eval_samples_per_second': 12.849, 'eval_steps_per_second': 6.425, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 1500/2250 [29:30<13:37,  1.09s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7461, 'grad_norm': 6.9619598388671875, 'learning_rate': 6.666666666666667e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 67%|██████▋   | 1500/2250 [29:45<13:37,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.1306755542755127, 'eval_runtime': 15.4121, 'eval_samples_per_second': 12.977, 'eval_steps_per_second': 6.488, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2250/2250 [43:42<00:00,  1.18s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.3156, 'grad_norm': 7.015842437744141, 'learning_rate': 0.0, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 2250/2250 [43:57<00:00,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.0383048057556152, 'eval_runtime': 15.4603, 'eval_samples_per_second': 12.936, 'eval_steps_per_second': 6.468, 'epoch': 3.0}\n",
      "{'train_runtime': 2637.7969, 'train_samples_per_second': 1.706, 'train_steps_per_second': 0.853, 'train_loss': 4.783296440972222, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2250, training_loss=4.783296440972222, metrics={'train_runtime': 2637.7969, 'train_samples_per_second': 1.706, 'train_steps_per_second': 0.853, 'total_flos': 944662958469120.0, 'train_loss': 4.783296440972222, 'epoch': 3.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./mt5-npi-en/tokenizer_config.json',\n",
       " './mt5-npi-en/special_tokens_map.json',\n",
       " './mt5-npi-en/spiece.model',\n",
       " './mt5-npi-en/added_tokens.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"./mt5-npi-en\")\n",
    "tokenizer.save_pretrained(\"./mt5-npi-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:17<00:00,  5.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Evaluation: {'eval_loss': 3.1013731956481934, 'eval_runtime': 17.812, 'eval_samples_per_second': 11.228, 'eval_steps_per_second': 5.614, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_results = trainer.evaluate(eval_dataset=tokenized_dataset[\"test\"])\n",
    "print(\"Test Evaluation:\", test_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating translations: 100%|██████████| 25/25 [02:52<00:00,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Source    : यसबाट युवकहरु पनि पीडित भएको देखिन्छन ।\n",
      "Reference : Young people also suffer from it.\n",
      "Predicted : It has also been damaged by youths.\n",
      "\n",
      "Source    : बजाऊने लिस्ट\n",
      "Reference : Playlist\n",
      "Predicted : It is a new folder\n",
      "\n",
      "Source    : त्यहाँ केही नियमहरू छन् जुन तपाईंले पछ्याउनु पर्छः\n",
      "Reference : There are a few rules that you should follow:\n",
      "Predicted : He doesn't know what you need to follow.\n",
      "\n",
      "Source    : सबै गीतहरु उनी आफैंले लेखेका हुन् ।\n",
      "Reference : All songs were written by himself.\n",
      "Predicted : He wrote a song.\n",
      "\n",
      "Source    : मेरो एउटा जर्मन साथी छ ।\n",
      "Reference : I had a German friend.\n",
      "Predicted : He is a foreigner.\n",
      "\n",
      "Source    : तपाईँ वास्तवमै फाइल मेट्न चाहनुहुन्छ?\n",
      "\n",
      "Reference : Do you really want to delete file ?\n",
      "\n",
      "Predicted : It is a mistake for you?\n",
      "\n",
      "Source    : यस कुरालाई लिएर आक्रोशित हुनुपर्ने कुनै आवश्यकता छैन ।\n",
      "Reference : There is no need to get upset about this.\n",
      "Predicted : It is not necessary to remove this problem.\n",
      "\n",
      "Source    : पुरुष र महिला दुबैले यसको लाभ उठाउन सक्छन्।\n",
      "Reference : Both women and men can take advantage of them.\n",
      "Predicted : It can help women and women.\n",
      "\n",
      "Source    : तपाईं त नर्तक हुनुहुन्थ्यो, गीत कसरी गाउन थाल्नुभयो ?\n",
      "Reference : You started off as a dancer, how did you end up singing?\n",
      "Predicted : It was not a singer?\n",
      "\n",
      "Source    : आंशिक खोजी नतिजाहरू (बढोत्तरित खोजी) देखाउनुहोस्\n",
      "Reference : Show partial search results (incremental search)\n",
      "Predicted : He displays a few changes in a few steps.\n",
      "Test BLEU: 0.03350703812600921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# For single-GPU CPU/accelerator inference\n",
    "model.eval()\n",
    "\n",
    "batch_size = 8  # You can tune this\n",
    "sources = dataset[\"test\"][\"source\"]\n",
    "references = dataset[\"test\"][\"target\"]\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for i in tqdm(range(0, len(sources), batch_size), desc=\"Generating translations\"):\n",
    "    batch_src = sources[i:i + batch_size]\n",
    "    batch_inputs = [\"translate Nepali to English: \" + s for s in batch_src]\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        batch_inputs,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=300\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=300,\n",
    "            num_beams=4\n",
    "        )\n",
    "    \n",
    "    batch_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    predictions.extend(batch_preds)\n",
    "\n",
    "# Print a few samples to check translation quality\n",
    "for i in range(10):\n",
    "    print(f\"\\nSource    : {sources[i]}\")\n",
    "    print(f\"Reference : {references[i]}\")\n",
    "    print(f\"Predicted : {predictions[i]}\")\n",
    "\n",
    "bleu_score = bleu.compute(\n",
    "    predictions=[p.strip() for p in predictions],\n",
    "    references=[[r.strip()] for r in references]\n",
    ")\n",
    "print(\"Test BLEU:\", bleu_score[\"bleu\"])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
